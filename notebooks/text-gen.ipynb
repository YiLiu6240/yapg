{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertLMHeadModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import settings\n",
    "from funcs.data_module import DataModule\n",
    "from funcs.utils import find_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = find_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = settings.chinese_bert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizerFast at 0x7f32d577e650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertLMHeadModel.from_pretrained(MODEL_NAME, return_dict=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-20 21:09:48.100 | INFO     | funcs.data_module:__init__:46 - data module hparams: {'max_tokenization_length': 64, 'batch_size': 64, 'num_workers': 2, 'min_word_frequency': 8}\n",
      "2020-10-20 21:09:48.102 | INFO     | funcs.data_module:setup:49 - Loading train dataset\n",
      "2020-10-20 21:09:48.104 | INFO     | funcs.data_module:get_dataset:84 - load from path: /work/ik18445/projects/yapg/datasets/output/poetry_64.pt\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 101, 2526,  704,  ...,    0,    0,    0],\n",
       "         [ 101, 5874, 2526,  ...,    0,    0,    0],\n",
       "         [ 101,  736, 4371,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 1066, 6457,  ...,    0,    0,    0],\n",
       "         [ 101, 1760, 4128,  ...,    0,    0,    0],\n",
       "         [ 101, 5428, 7731,  ...,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = data_module.train_dataloader()\n",
    "print(len(data_loader))\n",
    "for idx, batch in enumerate(data_loader):\n",
    "    if idx == 2:\n",
    "        data_batch = batch\n",
    "        break\n",
    "data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(7.4247, grad_fn=<NllLossBackward>), logits=tensor([[[ -7.4897,  -7.3078,  -7.3892,  ...,  -6.1555,  -6.8452,  -6.3288],\n",
       "         [ -7.0119,  -6.8831,  -6.8382,  ...,  -5.5850,  -5.7504,  -5.4461],\n",
       "         [ -8.4322,  -8.9203,  -9.2589,  ...,  -6.0365,  -7.4740,  -3.4020],\n",
       "         ...,\n",
       "         [ -6.7556,  -6.7236,  -7.0349,  ...,  -5.6988,  -7.0706,  -5.7675],\n",
       "         [ -6.5486,  -6.3824,  -6.7158,  ...,  -5.6458,  -7.0196,  -5.6393],\n",
       "         [ -7.0738,  -6.9013,  -7.3119,  ...,  -5.9904,  -6.8210,  -5.0939]],\n",
       "\n",
       "        [[ -7.1489,  -7.0483,  -7.0265,  ...,  -6.4867,  -6.8877,  -5.7832],\n",
       "         [ -7.2843,  -7.1744,  -7.1128,  ...,  -6.1080,  -6.5044,  -5.4240],\n",
       "         [ -6.2073,  -6.2613,  -6.3986,  ...,  -6.2980,  -7.1251,  -5.5918],\n",
       "         ...,\n",
       "         [ -5.5722,  -5.4419,  -5.5996,  ...,  -5.3593,  -6.5668,  -3.6356],\n",
       "         [ -5.7025,  -5.5547,  -5.6267,  ...,  -5.2922,  -6.1632,  -4.5116],\n",
       "         [ -5.6294,  -5.3395,  -5.4290,  ...,  -5.1185,  -6.1614,  -4.2697]],\n",
       "\n",
       "        [[ -7.7638,  -7.5514,  -7.6681,  ...,  -6.2660,  -6.1675,  -6.0798],\n",
       "         [ -7.7621,  -7.8491,  -7.8025,  ...,  -6.6668,  -6.5986,  -6.1106],\n",
       "         [ -9.8097, -10.0382, -10.4130,  ...,  -8.5863,  -9.1628,  -8.9068],\n",
       "         ...,\n",
       "         [ -6.8521,  -6.6258,  -6.9178,  ...,  -6.0498,  -7.3293,  -5.8047],\n",
       "         [ -6.9087,  -6.6484,  -6.9971,  ...,  -5.3336,  -6.5722,  -5.1900],\n",
       "         [ -7.4966,  -7.1969,  -7.6453,  ...,  -5.7423,  -6.3689,  -4.8282]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ -7.6832,  -7.6788,  -7.6185,  ...,  -7.1439,  -7.6877,  -6.8086],\n",
       "         [ -7.6715,  -7.5479,  -7.4327,  ...,  -6.3493,  -6.1662,  -5.6670],\n",
       "         [ -8.2454,  -7.9914,  -7.6178,  ...,  -4.4328,  -2.7647,  -3.9938],\n",
       "         ...,\n",
       "         [ -7.3171,  -7.4143,  -7.1573,  ...,  -6.4565,  -6.3503,  -6.4923],\n",
       "         [ -7.7365,  -7.8755,  -7.7200,  ...,  -6.2115,  -5.6741,  -5.0057],\n",
       "         [ -7.4277,  -7.4887,  -7.3450,  ...,  -7.0112,  -6.6517,  -5.9578]],\n",
       "\n",
       "        [[ -7.7500,  -7.7686,  -7.7400,  ...,  -7.0552,  -7.3369,  -7.1536],\n",
       "         [ -7.7681,  -7.7147,  -7.7344,  ...,  -6.6858,  -6.5820,  -6.0412],\n",
       "         [-13.1439, -13.4692, -13.4889,  ..., -12.1949, -12.2289,  -8.3786],\n",
       "         ...,\n",
       "         [ -8.6603,  -9.0044,  -8.7419,  ...,  -7.5469,  -8.6216,  -7.5580],\n",
       "         [ -8.6113,  -9.1049,  -8.7518,  ...,  -7.7713,  -8.1125,  -7.3089],\n",
       "         [ -8.4512,  -8.9080,  -8.5806,  ...,  -7.0079,  -6.0974,  -5.3582]],\n",
       "\n",
       "        [[ -7.7387,  -7.7653,  -7.6484,  ...,  -7.3955,  -7.4899,  -7.1553],\n",
       "         [ -7.6814,  -7.6296,  -7.5108,  ...,  -6.8754,  -6.6936,  -6.3045],\n",
       "         [ -9.6057,  -9.2901,  -8.7259,  ..., -10.2948,  -9.2612,  -9.2504],\n",
       "         ...,\n",
       "         [ -9.5056,  -9.5956,  -8.7914,  ...,  -9.0654,  -7.4478,  -8.1063],\n",
       "         [ -8.4590,  -8.3717,  -7.8213,  ...,  -8.1543,  -7.1206,  -7.7273],\n",
       "         [ -9.4145,  -9.6334,  -8.8601,  ...,  -8.9900,  -8.5468,  -9.2516]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": data_batch[0],\n",
    "    \"attention_mask\": data_batch[1],\n",
    "    \"token_type_ids\": data_batch[2],\n",
    "    \"labels\": data_batch[0],\n",
    "}\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4495, 3833, 3221,  103,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = \"生活是\"\n",
    "sentence = f\"{TEXT}{tokenizer.mask_token}\"\n",
    "encodings = tokenizer(sentence, return_tensors=\"pt\")\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": encodings[\"input_ids\"],\n",
    "    \"attention_mask\": encodings[\"attention_mask\"],\n",
    "    \"token_type_ids\": encodings[\"token_type_ids\"],\n",
    "    \"labels\": encodings[\"input_ids\"],\n",
    "}\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index = torch.where(encodings[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.9595, -7.8608, -7.9207,  ..., -6.9487, -7.1159, -7.0181],\n",
       "         [-8.1631, -8.0284, -8.0136,  ..., -6.9275, -7.1323, -6.6380],\n",
       "         [-8.7900, -8.6704, -8.5597,  ..., -7.0898, -8.7183, -7.7622],\n",
       "         [-8.2436, -8.1421, -8.2977,  ..., -6.1147, -6.1333, -5.7758],\n",
       "         [-7.8990, -7.5713, -7.6846,  ..., -4.6256, -5.5561, -4.9925],\n",
       "         [-9.0508, -9.1447, -9.2875,  ..., -7.6092, -8.1554, -8.0123]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs[\"logits\"]\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.8990, -7.5713, -7.6846,  ..., -4.6256, -5.5561, -4.9925]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_logits = logits[0, mask_token_index, :].detach()\n",
    "mask_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ik18445/miniconda3/envs/yapg/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "K = 30\n",
    "top_k = torch.topk(mask_token_logits, K, dim=1)\n",
    "top_k_logits = top_k[0][0]\n",
    "top_k_probs = torch.nn.functional.softmax(top_k_logits)\n",
    "top_k_indices = top_k[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 生活是。\tprob: 0.6666449308395386\n",
      "sentence: 生活是！\tprob: 0.18476134538650513\n",
      "sentence: 生活是？\tprob: 0.14617371559143066\n",
      "sentence: 生活是，\tprob: 0.0006420866120606661\n",
      "sentence: 生活是...\tprob: 0.00035333880805410445\n",
      "sentence: 生活是：\tprob: 0.00031461031176149845\n",
      "sentence: 生活是人\tprob: 0.0001470243587391451\n",
      "sentence: 生活是吗\tprob: 0.00014102619024924934\n",
      "sentence: 生活是、\tprob: 0.00013337582640815526\n",
      "sentence: 生活是我\tprob: 8.762655488681048e-05\n",
      "sentence: 生活是嗎\tprob: 6.456378469010815e-05\n",
      "sentence: 生活是:\tprob: 6.07806105108466e-05\n",
      "sentence: 生活是.\tprob: 5.6154309277189896e-05\n",
      "sentence: 生活是,\tprob: 4.761806849273853e-05\n",
      "sentence: 生活是你\tprob: 3.793793803197332e-05\n",
      "sentence: 生活是\"\tprob: 3.672907405416481e-05\n",
      "sentence: 生活是的\tprob: 2.9054008336970583e-05\n",
      "sentence: 生活是-\tprob: 2.8676879082922824e-05\n",
      "sentence: 生活是在\tprob: 2.6001134756370448e-05\n",
      "sentence: 生活是家\tprob: 2.5144174287561327e-05\n",
      "sentence: 生活是愛\tprob: 2.396278068772517e-05\n",
      "sentence: 生活是爱\tprob: 2.3668275389354676e-05\n",
      "sentence: 生活是么\tprob: 2.239015157101676e-05\n",
      "sentence: 生活是|\tprob: 1.9329076167196035e-05\n",
      "sentence: 生活是学\tprob: 1.932057784870267e-05\n",
      "sentence: 生活是是\tprob: 1.768529909895733e-05\n",
      "sentence: 生活是!\tprob: 1.6257352399406955e-05\n",
      "sentence: 生活是笑\tprob: 1.5634244846296497e-05\n",
      "sentence: 生活是他\tprob: 1.5236000763252378e-05\n",
      "sentence: 生活是好\tprob: 1.4800229109823704e-05\n"
     ]
    }
   ],
   "source": [
    "for idx, token in enumerate(top_k_indices.tolist()):\n",
    "    filled_sentence = sentence.replace(tokenizer.mask_token, tokenizer.decode([token]))\n",
    "    prob = top_k_probs[idx]\n",
    "    print(f\"sentence: {filled_sentence}\\tprob: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1962, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(tokenizer.convert_ids_to_tokens(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: a \"poem\" generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(encodings):\n",
    "    inputs = {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"token_type_ids\": encodings[\"token_type_ids\"],\n",
    "        \"labels\": encodings[\"input_ids\"],\n",
    "    }\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[\"logits\"].detach()\n",
    "    return logits\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    tokenizer: BertTokenizerFast,\n",
    "    model: BertLMHeadModel,\n",
    "    starting_text: str,\n",
    "    max_doc_length: int = 64,\n",
    "    max_tokenization_length: int = 128,\n",
    "    top_k: int = 100,\n",
    "):\n",
    "\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    # starting_encodings = tokenizer(\n",
    "    #     starting_text,\n",
    "    #     truncation=True,\n",
    "    #     padding=\"max_length\",\n",
    "    #     max_length=max_tokenization_length,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    target_text = starting_text\n",
    "    # target_ids = starting_encodings[\"input_ids\"]\n",
    "    i = 0\n",
    "    while i < max_doc_length:\n",
    "        print(\"run:\", i)\n",
    "        # print(target_ids)\n",
    "        # print(tokenizer.decode(target_ids))\n",
    "        encode_text = target_text + tokenizer.mask_token\n",
    "        # encodings = tokenizer(target_text, return_tensors=\"pt\")\n",
    "        # encode_ids = target_ids + [mask_token_id]\n",
    "        encodings = tokenizer(\n",
    "            # tokenizer.convert_ids_to_tokens(encode_ids),\n",
    "            encode_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_tokenization_length,\n",
    "        )\n",
    "        logits = get_logits(encodings)\n",
    "        # print(logits)\n",
    "        mask_token_index = torch.where(\n",
    "            encodings[\"input_ids\"] == tokenizer.mask_token_id\n",
    "        )[1]\n",
    "        mask_token_logits = logits[0, mask_token_index, :]\n",
    "        # print(mask_token_logits)\n",
    "        top_k_candidates = torch.topk(mask_token_logits, top_k, dim=1)\n",
    "        # try:\n",
    "        #     top_k_logits = top_k_candidates[0][0]\n",
    "        # except:\n",
    "        #     print(mask_token_logits)\n",
    "        #     print(top_k_candidates)\n",
    "        #     break\n",
    "        top_k_logits = top_k_candidates[0][0]\n",
    "        top_k_probs = torch.nn.functional.softmax(top_k_logits).numpy()\n",
    "        top_k_ids = top_k_candidates[1][0].tolist()\n",
    "        # randomly select an elem from top_k_ids, based on their softmax prob\n",
    "        target_id = np.random.choice(top_k_ids, p=top_k_probs)\n",
    "        print(target_id)\n",
    "        target_text = target_text + tokenizer.decode([target_id])\n",
    "        print(target_text)\n",
    "        i = i + 1\n",
    "    # target_text = tokenizer.decode(target_ids)\n",
    "    return target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ik18445/miniconda3/envs/yapg/lib/python3.7/site-packages/ipykernel_launcher.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n",
      "冬天。\n",
      "run: 1\n",
      "511\n",
      "冬天。。\n",
      "run: 2\n",
      "511\n",
      "冬天。。。\n",
      "run: 3\n",
      "511\n",
      "冬天。。。。\n",
      "run: 4\n",
      "511\n",
      "冬天。。。。。\n",
      "run: 5\n",
      "511\n",
      "冬天。。。。。。\n",
      "run: 6\n",
      "511\n",
      "冬天。。。。。。。\n",
      "run: 7\n",
      "511\n",
      "冬天。。。。。。。。\n",
      "run: 8\n",
      "511\n",
      "冬天。。。。。。。。。\n",
      "run: 9\n",
      "511\n",
      "冬天。。。。。。。。。。\n",
      "run: 10\n",
      "511\n",
      "冬天。。。。。。。。。。。\n",
      "run: 11\n",
      "511\n",
      "冬天。。。。。。。。。。。。\n",
      "run: 12\n",
      "511\n",
      "冬天。。。。。。。。。。。。。\n",
      "run: 13\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。\n",
      "run: 14\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。\n",
      "run: 15\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。\n",
      "run: 16\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。\n",
      "run: 17\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。\n",
      "run: 18\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。\n",
      "run: 19\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。\n",
      "run: 20\n",
      "519\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「\n",
      "run: 21\n",
      "6956\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部\n",
      "run: 22\n",
      "7386\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊\n",
      "run: 23\n",
      "520\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」\n",
      "run: 24\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。\n",
      "run: 25\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。\n",
      "run: 26\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。\n",
      "run: 27\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。\n",
      "run: 28\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。\n",
      "run: 29\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。\n",
      "run: 30\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。\n",
      "run: 31\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。\n",
      "run: 32\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。\n",
      "run: 33\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。\n",
      "run: 34\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。\n",
      "run: 35\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。\n",
      "run: 36\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。\n",
      "run: 37\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。\n",
      "run: 38\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。\n",
      "run: 39\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。\n",
      "run: 40\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。\n",
      "run: 41\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。\n",
      "run: 42\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。\n",
      "run: 43\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。\n",
      "run: 44\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 45\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 46\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 47\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 48\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 49\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 50\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 51\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。\n",
      "run: 52\n",
      "118\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。-\n",
      "run: 53\n",
      "118\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。--\n",
      "run: 54\n",
      "118\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。---\n",
      "run: 55\n",
      "118\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----\n",
      "run: 56\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。\n",
      "run: 57\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。\n",
      "run: 58\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。\n",
      "run: 59\n",
      "6963\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。都\n",
      "run: 60\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。都。\n",
      "run: 61\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。都。。\n",
      "run: 62\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。都。。。\n",
      "run: 63\n",
      "511\n",
      "冬天。。。。。。。。。。。。。。。。。。。。「部隊」。。。。。。。。。。。。。。。。。。。。。。。。。。。。----。。。都。。。。\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"冬天\"\n",
    "text = generate_text(tokenizer=tokenizer, model=model, starting_text=starting_text,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
